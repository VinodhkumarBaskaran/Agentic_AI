{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10bd0bc80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10c762ff0> root_client=<openai.OpenAI object at 0x10bb5f6e0> root_async_client=<openai.AsyncOpenAI object at 0x10bd0a0f0> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"RL\" can refer to several different concepts depending on the context. Here are some of the most common meanings:\\n\\n1. **Reinforcement Learning (RL):**\\n   - **Definition:** A branch of machine learning where an agent learns to make decisions by performing actions in an environment to achieve maximum cumulative rewards.\\n   - **How It Works:** The agent interacts with its environment by taking actions, receives feedback in the form of rewards or penalties, and uses this information to improve its future actions.\\n   - **Applications:** Robotics, game playing (e.g., AlphaGo), autonomous vehicles, recommendation systems, and more.\\n\\n2. **Real Life (RL):**\\n   - **Definition:** Refers to the physical world as opposed to virtual or online environments.\\n   - **Usage:** Commonly used in online communities and gaming to differentiate between experiences that occur offline versus those that happen in digital spaces.\\n   - **Example:** \"I meet my online friends in RL on weekends.\"\\n\\n3. **Rocket League (RL):**\\n   - **Definition:** A popular video game that combines elements of soccer with rocket-powered cars.\\n   - **Gameplay:** Players control cars to hit a large ball into the opposing team\\'s goal to score points.\\n   - **Popularity:** Known for its competitive gameplay and has a significant presence in eSports.\\n\\n4. **RL as Initials or Abbreviations:**\\n   - **Examples:**\\n     - **Release (RL):** Often used in project management or software development contexts.\\n     - **Registered Nurse (RN) vs. RL (rarely used):** Though \"RN\" is standard, \"RL\" might occasionally appear in specific contexts.\\n     - **Companies or Brands:** Some brands or organizations might use \"RL\" as an abbreviation (e.g., Ralph Lauren, although it\\'s commonly abbreviated as \"RL\").\\n\\n5. **Railway Locomotive (RL):**\\n   - **Definition:** In some contexts, especially in transportation and logistics, \"RL\" might refer to railway locomotives or related terms.\\n\\n**Which \"RL\" Are You Referring To?**\\n\\nIf you\\'re referring to a specific context not covered above, please provide more details so I can give a more accurate explanation!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 779, 'prompt_tokens': 11, 'total_tokens': 790, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_7989eaacf6', 'id': 'chatcmpl-BbePIh7vgDlCld9361Tz6JL7hYgW5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--89d5c5dd-d686-4c14-b9f1-1b3035d802c6-0' usage_metadata={'input_tokens': 11, 'output_tokens': 779, 'total_tokens': 790, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is RL\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"RL\" can refer to several different concepts depending on the context. Here are some of the most common meanings:\n",
      "\n",
      "1. **Reinforcement Learning (RL):**\n",
      "   - **Definition:** A branch of machine learning where an agent learns to make decisions by performing actions in an environment to achieve maximum cumulative rewards.\n",
      "   - **How It Works:** The agent interacts with its environment by taking actions, receives feedback in the form of rewards or penalties, and uses this information to improve its future actions.\n",
      "   - **Applications:** Robotics, game playing (e.g., AlphaGo), autonomous vehicles, recommendation systems, and more.\n",
      "\n",
      "2. **Real Life (RL):**\n",
      "   - **Definition:** Refers to the physical world as opposed to virtual or online environments.\n",
      "   - **Usage:** Commonly used in online communities and gaming to differentiate between experiences that occur offline versus those that happen in digital spaces.\n",
      "   - **Example:** \"I meet my online friends in RL on weekends.\"\n",
      "\n",
      "3. **Rocket League (RL):**\n",
      "   - **Definition:** A popular video game that combines elements of soccer with rocket-powered cars.\n",
      "   - **Gameplay:** Players control cars to hit a large ball into the opposing team's goal to score points.\n",
      "   - **Popularity:** Known for its competitive gameplay and has a significant presence in eSports.\n",
      "\n",
      "4. **RL as Initials or Abbreviations:**\n",
      "   - **Examples:**\n",
      "     - **Release (RL):** Often used in project management or software development contexts.\n",
      "     - **Registered Nurse (RN) vs. RL (rarely used):** Though \"RN\" is standard, \"RL\" might occasionally appear in specific contexts.\n",
      "     - **Companies or Brands:** Some brands or organizations might use \"RL\" as an abbreviation (e.g., Ralph Lauren, although it's commonly abbreviated as \"RL\").\n",
      "\n",
      "5. **Railway Locomotive (RL):**\n",
      "   - **Definition:** In some contexts, especially in transportation and logistics, \"RL\" might refer to railway locomotives or related terms.\n",
      "\n",
      "**Which \"RL\" Are You Referring To?**\n",
      "\n",
      "If you're referring to a specific context not covered above, please provide more details so I can give a more accurate explanation!\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Agentic AI QAing metrics are a set of metrics used to evaluate the quality of answers generated by agentic AI systems. Agentic AI systems are AI systems that can act autonomously and make decisions on their own.\\n\\n**Key Metrics:**\\n\\n* **Accuracy:** The percentage of answers that are factually correct and aligned with the ground truth.\\n* **Completeness:** The extent to which the answers provide all the necessary information to address the user's query.\\n* **Relevance:** The degree to which the answers are directly related to the user's query and avoid off-topic or irrelevant information.\\n* **Coherence and Fluency:** The grammatical correctness, logical flow, and readability of the generated answers.\\n* **Bias and Fairness:** The absence of discriminatory or prejudiced language or content in the answers.\\n* **Safety and Ethical Considerations:**\\n\\nThe answers should not promote harmful, unethical, or illegal activities.\\n* **Engagement and Naturalness:** The answers should be engaging, conversational, and sound like they were produced by a human.\\n\\n**Challenges in Measuring Agentic AI QA:**\\n\\n* **Subjectivity:** Some aspects of quality, such as engagement and naturalness, can be subjective and difficult to quantify.\\n* **Contextual Understanding:** Agentic AI systems often operate in complex and dynamic contexts, making it challenging to assess their responses accurately.\\n* **Evolving Nature of AI:** The field of AI is rapidly evolving, with new techniques and models constantly emerging. This makes it difficult to establish standardized metrics that are applicable across all agentic AI systems.\\n\\n**Approaches to Evaluation:**\\n\\n* **Human Evaluation:**\\n\\nExperts or crowdsourced annotators assess the quality of AI-generated answers based on predefined criteria.\\n* **Automatic Metrics:**\\n\\nAlgorithms are used to automatically measure aspects such as accuracy, fluency, and bias.\\n* **Benchmark Datasets:**\\n\\nStandardized datasets with labeled examples are used to compare the performance of different AI systems.\\n\\n\\nIt's important to note that a single metric is rarely sufficient to capture the full complexity of agentic AI QA. A comprehensive evaluation typically involves a combination of metrics and human judgment.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 445, 'prompt_tokens': 18, 'total_tokens': 463, 'completion_time': 0.809090909, 'prompt_time': 0.002129043, 'queue_time': 0.35856572400000003, 'total_time': 0.811219952}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--5909b80a-2892-4587-b1e4-a45dd1f93614-0', usage_metadata={'input_tokens': 18, 'output_tokens': 445, 'total_tokens': 463})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model.invoke(\"Tell me about Agentic AI QAing metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81a8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(\"Tell me about Agentic AI QAing metrics\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10cb585c0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10cb63ad0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10cb585c0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10cb63ad0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I'm familiar with Langsmith! \n",
      "\n",
      "Langsmith is an **open-source tool** focused on making it **easier to build and deploy AI applications**.  \n",
      "\n",
      "Here are some key things to know about it:\n",
      "\n",
      "* **Framework Agnostic:** Langsmith doesn't tie you to a specific AI framework like TensorFlow or PyTorch. It works with various popular ones, giving you flexibility.\n",
      "* **Simplified Deployment:** It streamlines the process of deploying your AI models, whether to the cloud or on your own hardware. \n",
      "* **Focus on User Experience:** Langsmith aims to provide a user-friendly interface and experience, making AI development more accessible to a wider range of developers.\n",
      "* **Community Driven:** Being open-source, Langsmith benefits from a vibrant community that contributes to its development, documentation, and support.\n",
      "\n",
      "**Here are some specific features Langsmith offers:**\n",
      "\n",
      "* **Model Management:** Easily manage your AI models, including versioning, tracking, and deployment.\n",
      "* **Prompt Engineering Tools:**  Provides tools to help you craft effective prompts for your AI models.\n",
      "* **Pipeline Creation:** Build complex AI workflows by chaining together different models and tasks.\n",
      "* **Monitoring and Logging:** Track the performance and health of your deployed AI applications.\n",
      "\n",
      "**If you're interested in learning more, here are some resources:**\n",
      "\n",
      "* **Official Website:** [https://github.com/google/langsmith](https://github.com/google/langsmith)\n",
      "* **Documentation:** [https://langsmith.dev/docs/](https://langsmith.dev/docs/)\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or AI development in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me tell you about LangSmith!\n",
      "\n",
      "LangSmith is an open-source framework developed by the Hugging Face team specifically designed for **fine-tuning large language models (LLMs)**. Think of it as a powerful toolbox for customizing these models to your specific needs.\n",
      "\n",
      "Here's a breakdown of its key features and benefits:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Simplified Fine-Tuning:** LangSmith streamlines the process of fine-tuning LLMs, making it more accessible to a wider range of users, even those without extensive machine learning expertise.\n",
      "\n",
      "* **Data Management:** It provides tools for efficiently managing and preparing your training data, ensuring it's in the right format for fine-tuning.\n",
      "\n",
      "* **Experiment Tracking:** LangSmith helps you keep track of your fine-tuning experiments, allowing you to compare different model configurations, hyperparameters, and datasets. This makes it easier to identify the best performing models.\n",
      "* **Collaborative Development:** As an open-source project, LangSmith fosters a collaborative environment where developers can share their work, contribute to the framework, and learn from each other.\n",
      "\n",
      "* **Integration with Hugging Face Ecosystem:** LangSmith seamlessly integrates with the Hugging Face Hub, a vast repository of pre-trained LLMs and datasets. This means you can easily access and fine-tune models from the community or share your own creations.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Customization:** Fine-tune LLMs to excel at specific tasks, such as text classification, question answering, summarization, or code generation.\n",
      "\n",
      "* **Improved Performance:** Achieve better results on your target tasks by tailoring the model to your unique data and requirements.\n",
      "* **Efficiency:** Streamline the fine-tuning process with user-friendly tools and automation.\n",
      "* **Transparency and Reproducibility:**  LangSmith's open-source nature promotes transparency and allows others to reproduce your experiments.\n",
      "\n",
      "**In summary,** LangSmith empowers you to customize and enhance the capabilities of LLMs, making them more effective for a wide range of applications. Its user-friendly interface, collaborative spirit, and integration with the Hugging Face ecosystem make it a valuable resource for AI developers and researchers alike.\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about LangSmith or want to explore specific use cases!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying AI assistants powered by large language models (LLMs).', 'key_features': ['Modular and extensible architecture', 'Intuitive visual interface for building conversational flows', 'Support for multiple LLMs, including open-source and commercial options', 'Tools for fine-tuning LLMs on custom datasets', 'Deployment options for web, mobile, and embedded platforms'], 'benefits': ['Faster development cycles for AI assistants', 'Increased customization and control over AI behavior', 'Reduced costs compared to proprietary solutions', 'Access to a growing community of developers and users'], 'website': 'https://www.langsmith.com/'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"Langsmith is an open-source platform created by the team at Cohere designed to simplify the process of building and deploying custom AI applications.  \\n\\nHere are some key things to know about Langsmith:\\n\\n* **Focus on User Experience:** Langsmith prioritizes making AI development accessible to a wider audience, even those without extensive coding experience.\\n\\n* **Modular and Customizable:** It offers a modular architecture that allows users to easily integrate different AI models, tools, and components to create tailored solutions.\\n* **Streamlined Workflow:** Langsmith streamlines the development workflow by providing intuitive interfaces for tasks like data preparation, model training, and model deployment.\\n* **Emphasis on Collaboration:**  The platform encourages collaboration through features that enable teams to work together on AI projects.\\n* **Open Source:**  Being open-source, Langsmith fosters community contributions, transparency, and the ability to adapt the platform to specific needs.\\n\\n**Key Features:**\\n\\n* **Model Hub:** A repository of pre-trained AI models from various providers, including Cohere's own models.\\n* **Data Playground:** A space for cleaning, transforming, and visualizing data used for training AI models.\\n* **Experiment Tracker:**  Tools to track and compare the performance of different AI models and training configurations.\\n* **Deployment Tools:**  Options for deploying trained models as APIs or web applications.\\n\\n**Benefits:**\\n\\n* **Reduced Development Time:**  Langsmith's streamlined workflow and pre-built components accelerate the development process.\\n* **Improved Accessibility:**  The platform lowers the barrier to entry for individuals and teams new to AI development.\\n* **Increased Flexibility:**  The modular design allows for customization and the integration of various AI technologies.\\n\\n**Getting Started:**\\n\\nYou can find more information about Langsmith, including documentation, tutorials, and community resources, on the official Cohere website or their GitHub repository.\"}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide a reponse in XML format for example <response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide a reponse in XML format for example <response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': [{'project': 'Langsmith'}, {'description': 'Langsmith is an open-source project focused on developing and deploying large language models (LLMs). It aims to provide a platform for researchers and developers to collaborate on building and experimenting with new LLM architectures, training methodologies, and applications.'}, {'features': [{'feature': 'Modular design allows for easy customization and experimentation.'}, {'feature': 'Supports various LLM architectures, including transformer-based models.'}, {'feature': 'Provides tools for training, evaluating, and deploying LLMs.'}, {'feature': 'Open-source and community-driven, fostering collaboration and innovation.'}]}, {'website': 'https://github.com/google/langhsmith'}]}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools and components to streamline the process of integrating LLMs into various applications, such as chatbots, question-answering systems, and text summarizers. </answer></response>' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 39, 'total_tokens': 108, 'completion_time': 0.125454545, 'prompt_time': 0.003350234, 'queue_time': 0.243215077, 'total_time': 0.128804779}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--0d9d3374-01e5-4474-b3a2-e9963c5226b6-0' usage_metadata={'input_tokens': 39, 'output_tokens': 69, 'total_tokens': 108}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle stand up by itself?\",\n",
       " 'punchline': 'Because it was two tired!'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatGroq(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the scarecrow win an award? Because he was outstanding in his field!'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Sully</movie>\n",
      "<movie>Toy Story</movie>\n",
      "<movie>Sleepless in Seattle</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999f98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "298456a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_name': 'MacBook',\n",
       " 'product_details': 'A line of premium laptop computers designed, developed, and marketed by Apple Inc.',\n",
       " 'price_usd': 1299}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\",temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class ProductInfo(BaseModel):\n",
    "    product_name: str = Field(..., description=\"Name of the product\")\n",
    "    product_details: str = Field(..., description=\"Brief description of the product\")\n",
    "    price_int: int = Field(..., description=\"Tentative price in INR\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=ProductInfo)\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "product_query = \"Tell me about macbook.\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=''' You are a helpful product assistant (Indian Market).\\n\n",
    "         Return output in the following JSON format:\\n\n",
    "         {\\n\"\n",
    "           \\\"product_name\\\": string,\\n\"\n",
    "          \\\"product_details\\\": string,\\n\"\n",
    "           \\\"price_usd\\\": integer (no currency symbol)\\n\"\n",
    "         }\\n\"\n",
    "         \"Respond only in this format.  ''',\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": product_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a67e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
